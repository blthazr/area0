---
alertmanager:
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: 1Gi
          storageClassName: ceph-block
  config:
    global:
      resolve_timeout: 5m
      slack_api_url: "${SECRET_ALERT_MANAGER_DISCORD_WEBHOOK}"
    inhibit_rules:
      - equal: ["alertname", "namespace"]
        source_matchers:
          - severity = "critical"
        target_matchers:
          - severity = "warning"
    receivers:
      - name: "null"
      - name: "pagerduty"
        pagerduty_configs:
          - description: |-
              {{ template "pagerduty.custom.description" . }}
            routing_key: "${SECRET_PAGERDUTY_TOKEN}"
      - name: "discord"
        slack_configs:
          - channel: "#prometheus-alerts"
            icon_url: https://avatars3.githubusercontent.com/u/3380462
            send_resolved: true
            text: >-
              {{ range .Alerts -}}
                **Alert:** {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}

              **Description:** {{ if ne .Annotations.description ""}}{{ .Annotations.description }}{{else}}N/A{{ end }}

              **Details:**
                {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
                {{ end }}
              {{ end }}
            title: |-
              [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if ne .CommonAnnotations.summary ""}}{{ .CommonAnnotations.summary }}{{ else }}{{ .CommonLabels.alertname }}{{ end }}
            username: "Prometheus"
    route:
      group_by: ["alertname", "job"]
      group_interval: 5m
      group_wait: 30s
      receiver: "discord"
      repeat_interval: 6h
      routes:
        - receiver: "null"
          matchers:
            - alertname =~ "InfoInhibitor|Watchdog"
        - receiver: "pagerduty"
          continue: true
          matchers:
            - severity = "critical"
        - receiver: "discord"
          continue: true
          matchers:
            - severity = "critical"
    templates: ["*.tmpl"]
  templateFiles:
    pagerduty-custom.tmpl: |-
      {{- define "pagerduty.custom.description" -}}[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if ne .CommonAnnotations.summary ""}}{{ .CommonAnnotations.summary }} {{ else if ne .CommonAnnotations.message ""}}{{ .CommonAnnotations.message }} {{ else if ne .CommonAnnotations.description ""}}{{ .CommonAnnotations.description }} {{ else }}{{ .CommonLabels.alertname }}{{ end }}{{- end -}}
  ingress:
    enabled: true
    hosts:
      - &host "alert-manager.${SECRET_DOMAIN}"
    ingressClassName: nginx
    pathType: Prefix
    tls:
      - hosts:
          - *host
grafana:
  enabled: false
  forceDeployDashboards: true
  sidecar:
    dashboards:
      multicluster:
        etcd:
          enabled: true
kube-state-metrics:
  metricLabelsAllowlist:
    - "persistentvolumeclaims=[*]"
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node
kubeApiServer:
  enabled: true
kubeControllerManager:
  enabled: true
  endpoints:
    - "${K3S_NODE_1_ADDRESS}"
    - "${K3S_NODE_2_ADDRESS}"
    - "${K3S_NODE_3_ADDRESS}"
  service:
    enabled: true
    port: 10257
    targetPort: 10257
kubeEtcd:
  enabled: true
  endpoints:
    - "${K3S_NODE_1_ADDRESS}"
    - "${K3S_NODE_2_ADDRESS}"
    - "${K3S_NODE_3_ADDRESS}"
  service:
    enabled: true
    port: 2381
    targetPort: 2381
kubeProxy:
  enabled: true
  endpoints:
    - "${K3S_NODE_1_ADDRESS}"
    - "${K3S_NODE_2_ADDRESS}"
    - "${K3S_NODE_3_ADDRESS}"
kubeScheduler:
  enabled: true
  endpoints:
    - "${K3S_NODE_1_ADDRESS}"
    - "${K3S_NODE_2_ADDRESS}"
    - "${K3S_NODE_3_ADDRESS}"
  service:
    enabled: true
    port: 10259
    targetPort: 10259
kubelet:
  enabled: true
  serviceMonitor:
    metricRelabelings:
      - action: replace
        sourceLabels:
          - node
        targetLabel: instance
prometheus:
  ingress:
    enabled: true
    hosts:
      - &host "prometheus.${SECRET_DOMAIN}"
    ingressClassName: nginx
    pathType: Prefix
    tls:
      - hosts:
          - *host
  prometheusSpec:
    additionalScrapeConfigs:
      - job_name: minio
        honor_timestamps: true
        metrics_path: /minio/v2/metrics/cluster
        scrape_interval: 1m
        scrape_timeout: 10s
        static_configs:
          - targets:
              - "${MINIO_ADDRESS}:9000"
      - job_name: node-exporter
        honor_timestamps: true
        scrape_interval: 1m
        scrape_timeout: 10s
        static_configs:
          - targets:
              - "${NAS_ADDRESS}:9100"
    enableAdminAPI: true
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    replicaExternalLabelName: replica
    replicas: 1
    resources:
      limits:
        memory: 6000Mi
      requests:
        cpu: 250m
        memory: 2000Mi
    retention: 6h
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ceph-block
          resources:
            requests:
              storage: 10Gi
    thanos:
      image: quay.io/thanos/thanos:v0.26.0
      objectStorageConfig:
        name: thanos-objstore-secret
        key: objstore.yml
      # renovate: datasource=docker depName=quay.io/thanos/thanos
      version: "v0.26.0"
    walCompression: true
  thanosService:
    enabled: true
  thanosServiceMonitor:
    enabled: true
